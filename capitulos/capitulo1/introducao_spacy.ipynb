{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca scaCY\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um objeto nlp vazio da lingua portuguesa\n",
    "nlp = spacy.blank(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criado após processar um texto com o objeto nlp\n",
    "doc = nlp(\"Olá mundo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá\n",
      "mundo\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Iterando nos tokens do doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexar o Doc para obter um Token\n",
    "token = doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mundo\n"
     ]
    }
   ],
   "source": [
    "# Obter o texto do token através do atributo.text\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Um pedaço do Doc é um objeto Particao (Span)\n",
    "span = doc[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mundo!\n"
     ]
    }
   ],
   "source": [
    "# Obter o texto da particao com atributo text\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atributos léxicos\n",
    "doc = nlp(\"Isso custa R$ 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Index: \", [token.i for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  ['Isso', 'custa', 'R$', '5']\n"
     ]
    }
   ],
   "source": [
    "print(\"Text: \", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_alpha:  [True, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "print(\"is_alpha: \", [token.is_alpha for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_punct:  [False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "print(\"is_punct: \", [token.is_punct for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linke_num:  [False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "print(\"linke_num: \", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeiros passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Como vai?\n"
     ]
    }
   ],
   "source": [
    "# spacy.blank para criar um objeto nlp vazio do idioma português\n",
    "import spacy\n",
    "\n",
    "# objeto nlp do portugues\n",
    "nlp = spacy.blank(\"pt\")\n",
    "\n",
    "# Processar o texto em portugues\n",
    "doc = nlp(\"Como vai?\")\n",
    "\n",
    "# imprimir o texto do documento\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentos, partições e tokens\n",
    "* Quando você chama o objeto nlp passando uma string como parâmetro, a spaCy faz a toquenização do texto e cria um objeto do tipo documento. Neste exercício, você vai aprender mais sobre o documento Doc, assim como os objetos Token e partição Span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"pt\")\n",
    "\n",
    "# Processar o texto\n",
    "doc = nlp(\"Eu gosto de gatos e cachorros.\")\n",
    "\n",
    "# Selecionar o primeiro token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Imprimir o texto do primeiro token\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "três cachorros\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"pt\")\n",
    "\n",
    "# Processar o texto\n",
    "doc = nlp(\"Eu tenho três cachorros e dois gatos\")\n",
    "\n",
    "# Uma partição do Doc para \"três cachorros\"\n",
    "tres_cachorros = doc[2:4]\n",
    "print(tres_cachorros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "três cachorros e dois gatos\n"
     ]
    }
   ],
   "source": [
    "# Uma particao do Doc para \"três cachorros e dois gatos\"\n",
    "tres_cachorros_dois_gatos = doc[2:7]\n",
    "print(tres_cachorros_dois_gatos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atributos léxicos\n",
    "\n",
    "* usar os objetos Doc e Token combinados com atributos léxicos para encontrar referências de porcentagem em seu texto. Procurar por dois elementos (tokens) sequenciais: um número e um sinal de porcentagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentuais encontrados:  60 %\n",
      "Percentuais encontrados:  4 %\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Processar o texto\n",
    "\n",
    "doc = nlp(\n",
    "    \"Em 1990, mais de 60% da população da Ásia Oriental estava em situação de extrema pobreza.\"\n",
    "    \"Agora, menos de 4% está nessa situação.\"  \n",
    ")\n",
    "\n",
    "# Iterar os tokens de um documento doc\n",
    "for token in doc:\n",
    "    # Checar se o token é composto por algarismo numéricos\n",
    "    if token.like_num:\n",
    "        # Selecionar o próximo token do documento\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Checar se o texto do proximo token é igal a \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentuais encontrados: \", token.text,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluxos (pipelines) de processamento treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pt-core-news-sm==3.7.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.7.0/pt_core_news_sm-3.7.0-py3-none-any.whl in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (3.7.0)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from pt-core-news-sm==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.66.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (24.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0; python_version < \"3.9\" in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.24.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.9.4)\n",
      "Requirement already satisfied: jinja2 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: setuptools in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (44.0.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: confection<0.2.0,>=0.0.4 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.11.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.18.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/tkroza/github/spacy_tutorial/.env/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.11)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os fluxos (pipelines) de processamento que estamos usando neste treinamento já vem pré-instalados. Para saber mais informações sobre os fluxos (pipelines) de processamento treinados e como instalá-los em seu computador, consulte essa documentação.\n",
    "\n",
    "    Utilize spacy.load para carregar o fluxo (pipeline) de processamento pequeno do idioma português \"pt_core_news_sm\".\n",
    "    Processe o texto e imprima o texto do documento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "É oficial: a Apple é a primeira empresa dos Estados Unidos a ter o valor de mercado acima de 1 trilhão.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Carregando o fluxo de processamento\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "text = \"É oficial: a Apple é a primeira empresa dos Estados Unidos a ter o valor de mercado acima de 1 trilhão.\"\n",
    "\n",
    "# Processe o texto\n",
    "doc = nlp(text)\n",
    "\n",
    "# imprimindo o atributo texto do documento\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prevendo Anotações linguísticas\n",
    "\n",
    "Agora vamos experimentar um dos fluxos (pipelines) de processamento treinados da biblioteca spaCy e ver o resultado de sua previsão. Fique à vontade e experimente com seu próprio texto! Use spacy.explain para saber o significado de um determinado marcador. Por exemplo: spacy.explain(\"PROPN\") ou spacy.explain(\"GPE\").\n",
    "Parte 1\n",
    "\n",
    "    Processe o texto utilizando o objeto nlp e crie um doc.\n",
    "    Para cada token, imprima seu texto, sua classe gramatical .pos_ e seu termo sintático .dep_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "É           AUX       cop       \n",
      "oficial     ADJ       ROOT      \n",
      ":           PUNCT     punct     \n",
      "a           DET       det       \n",
      "Apple       PROPN     nsubj     \n",
      "é           AUX       cop       \n",
      "a           DET       det       \n",
      "primeira    ADJ       amod      \n",
      "empresa     NOUN      csubj     \n",
      "dos         ADP       case      \n",
      "Estados     PROPN     nmod      \n",
      "Unidos      PROPN     flat:name \n",
      "a           SCONJ     mark      \n",
      "ter         VERB      acl       \n",
      "o           DET       det       \n",
      "valor       NOUN      obj       \n",
      "de          ADP       case      \n",
      "mercado     NOUN      nmod      \n",
      "acima       ADV       advmod    \n",
      "de          ADP       case      \n",
      "1           NUM       nummod    \n",
      "trilhão     NOUN      obl       \n",
      ".           PUNCT     punct     \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Carregando o fluxo de processamento\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "text = \"É oficial: a Apple é a primeira empresa dos Estados Unidos a ter o valor de mercado acima de 1 trilhão.\"\n",
    "\n",
    "# Processe o texto\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Selecionar o texto, a classe gramatical e o termo sintático de um token\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Estados Unidos LOC\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Carregando o fluxo de processamento\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "text = \"É oficial: a Apple é a primeira empresa dos Estados Unidos a ter o valor de mercado acima de 1 trilhão.\"\n",
    "\n",
    "# Processe o texto\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterando nas entidades previstas\n",
    "for ent in doc.ents:\n",
    "    # Imprimir o texto e a etiqueta da entidade\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prevendo Entidades em um contexto\n",
    "\n",
    "\n",
    "Os modelos são estatísticos e por isso não acertam 100% dos casos. A acurácia do modelo depende dos dados nos quais o modelo foi treinado e também dos dados que você está processando. Vamos ver um exemplo:\n",
    "\n",
    "    Processe o texto utilizando o objeto nlp.\n",
    "    Construa uma iteração nas entidades e imprima o texto e o marcador (label) da entidade.\n",
    "    Note que o modelo não previu “iPhone X”. Crie manualmente uma partição para esses tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X MISC\n",
      "Apple ORG\n",
      "Partição:  iPhone X\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "text = \"Vazou a data de lançamento do novo iPhone X após a Apple revelar a existência de compras antecipadas.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterando as entidades previstas\n",
    "for ent in doc.ents:\n",
    "    # Imprimir o texto e etiqueta (label) da entidade\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "# Selecionar a partição para iphone x\n",
    "iphone_x = doc[7:9]\n",
    "\n",
    "print(\"Partição: \", iphone_x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando comparador Matcher\n",
    "\n",
    "Vamos agora testar o comparador de expressões Matcher baseado em regras. Você vai usar o exemplo do exercício anterior e escrever uma expressão que faça a correspondência para a frase “iPhone X” no texto.\n",
    "\n",
    "    Importe o Matcher de spacy.matcher.\n",
    "    Inicialize o comparador com o objeto compartilhado vocabdo nlp.\n",
    "    Crie uma expressão que faça a correspondência dos valores em \"TEXT\" para dois tokens: \"iPhone\" e \"X\".\n",
    "    Use o método matcher.add e adicione essa expressão ao comparador.\n",
    "    Chame o comparador passando como parâmetro o doc e armazene o resultado na variável matches.\n",
    "    Itere nos resultados e selecione a partição de texto com o índice start até end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correspondências: ['iPhone X']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Comparador mathcer\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "doc = nlp(\"Vazou a data de lançamento do novo iPhone X após a Apple revelar a existência de compras antecipadas.\")\n",
    "\n",
    "# Inicialize o comparador com o vocabulario compartilhado\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Crie uma expressao que faça a correspondencia dos tokens iphone e x\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "\n",
    "# Adicionando uma expressao ao comparador\n",
    "matcher.add(\"IPHONE_X_PATTERN\", [pattern])\n",
    "\n",
    "# Use o comparador no doc\n",
    "matches = matcher(doc)\n",
    "print(\"Correspondências:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
